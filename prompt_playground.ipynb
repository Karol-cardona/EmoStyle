{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-28T12:40:22.816098Z",
     "start_time": "2025-05-28T12:40:22.811470Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from json.decoder import JSONDecodeError\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:40:25.990886Z",
     "start_time": "2025-05-28T12:40:22.835261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = BlipProcessor.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\",\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# load weights via safetensors format\n",
    "vision_model = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\",\n",
    "    use_safetensors=True\n",
    ").vision_model\n",
    "\n",
    "\n",
    "vision_model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    vision_model.cuda()\n",
    "\n",
    "def extract_image_features(image_path):\n",
    "    \"\"\"Return a single [hidden_size] vector for the input image.\"\"\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=img, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    outputs = vision_model(**inputs)\n",
    "    # take the first token (CLS) embedding\n",
    "    return outputs.last_hidden_state[:, 0, :].tolist()[0]"
   ],
   "id": "c7aaebd6ef5a32e8",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:40:26.646679Z",
     "start_time": "2025-05-28T12:40:26.038280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img_path = \"data/images/nm1055413_rm704041984_1977-4-2_2014.jpg\"\n",
    "feats = extract_image_features(img_path)\n",
    "print(\"Feature vector length:\", len(feats))"
   ],
   "id": "65f965bb01393172",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector length: 768\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:40:26.723147Z",
     "start_time": "2025-05-28T12:40:26.710488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def llama_generate(prompt: str,\n",
    "                   model: str = \"llama3.2\",\n",
    "                   temperature: float = 0.8,\n",
    "                   max_length: int = 40) -> str:\n",
    "\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_length\": max_length,\n",
    "        \"stream\":  True\n",
    "    }\n",
    "    res = requests.post(url, json=payload, stream=True)\n",
    "    res.raise_for_status()\n",
    "\n",
    "    full_text = []\n",
    "    for line in res.iter_lines(decode_unicode=True):\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "        # each JSON chunk has a \"response\" key\n",
    "        chunk = data.get(\"response\", \"\")\n",
    "        if chunk:\n",
    "            full_text.append(chunk)\n",
    "        # stop once the model signals it's done\n",
    "        if data.get(\"done\", False):\n",
    "            break\n",
    "\n",
    "    return \"\".join(full_text).strip()\n"
   ],
   "id": "7fcb00a6a5cfbc2a",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T12:40:32.302596Z",
     "start_time": "2025-05-28T12:40:26.768010Z"
    }
   },
   "cell_type": "code",
   "source": "print(llama_generate(\"Hello, my name is\", temperature=0.5, max_length=10))\n",
   "id": "306771fa877ad2bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm happy to chat with you. It seems like we just started our conversation and I haven't gotten a chance to know your name yet! Would you like to share it with me?\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
