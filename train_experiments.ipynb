{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-12T16:39:22.318875Z",
     "start_time": "2025-06-12T16:39:22.305163Z"
    }
   },
   "source": [
    "# 1.1 – Assicurati di aver installato requirements.txt e di avere training.py nel working dir\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# riduci la dimensione massima dei blocchi allocati per evitare frammentazione\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "from tqdm import TqdmWarning\n",
    "warnings.filterwarnings(\"ignore\", category=TqdmWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"You have not specified a value for the `type` parameter.*\"\n",
    ")\n",
    "\n",
    "sys.path.append(str(Path().resolve()))\n",
    "\n",
    "from training import run_lora_train\n",
    "\n",
    "import gradio as gr\n",
    "chatbot = gr.Chatbot(type=\"messages\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T16:39:23.097587Z",
     "start_time": "2025-06-12T16:39:23.085796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Current device index:\", torch.cuda.current_device())\n",
    "    print(\"Allocated (MB):\", torch.cuda.memory_allocated(0) / 1024**2)\n",
    "    print(\"Cached    (MB):\", torch.cuda.memory_reserved(0)  / 1024**2)"
   ],
   "id": "488ad5099c5e967d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device name: NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Current device index: 0\n",
      "Allocated (MB): 4996.25\n",
      "Cached    (MB): 7748.0\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T16:39:23.889527Z",
     "start_time": "2025-06-12T16:39:23.847005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Quick‐Experiment: LoRA su sottoinsieme\n",
    "\"\"\"\n",
    "Qui lanciamo `run_lora_train` su un sottoinsieme di 500 esempi per testare velocemente.\n",
    "\"\"\"\n",
    "\n",
    "# Configurazione esperimento rapido\n",
    "BASE_TRAIN = \"data/splits/train.json\"\n",
    "SUBSET_FILE = \"checkpoints/train_aug.json\"\n",
    "with open(BASE_TRAIN, \"r\", encoding=\"utf-8\") as f:\n",
    "    records = json.load(f)\n",
    "\n",
    "# creiamo subset di 500 esempi\n",
    "subset = records[:500]\n",
    "Path(\"data/splits\").mkdir(exist_ok=True, parents=True)\n",
    "with open(SUBSET_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(subset, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Subset con {len(subset)} esempi salvato in {SUBSET_FILE}\")\n"
   ],
   "id": "4b8e87f4e55d0edf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset con 500 esempi salvato in checkpoints/train_aug.json\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T16:41:01.135201Z",
     "start_time": "2025-06-12T16:40:57.692573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assicurati che la cartella esista\n",
    "exp_dir = Path(\"experiments/lora_subset\")\n",
    "exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Lancia LoRA sul subset con batch_size piccolo\n",
    "exp_dir = Path(\"experiments/lora_subset\")\n",
    "run_lora_train(\n",
    "    train_file=SUBSET_FILE,\n",
    "    output_dir=str(exp_dir),\n",
    "    model_name=\"huggyllama/llama-7b\",\n",
    "    seed=123,\n",
    "    use_4bit=True,\n",
    "    use_bfloat16=False,\n",
    "    batch_size=1\n",
    ")\n"
   ],
   "id": "ee0d5bcf610379a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA] Esempi da checkpoints\\train_aug.json: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at C:\\Users\\jinet\\.cache\\huggingface\\hub\\models--huggyllama--llama-7b\\snapshots\\4782ad278652c7c71b72204d462d6d01eaaf7549\\tokenizer.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\jinet\\.cache\\huggingface\\hub\\models--huggyllama--llama-7b\\snapshots\\4782ad278652c7c71b72204d462d6d01eaaf7549\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\jinet\\.cache\\huggingface\\hub\\models--huggyllama--llama-7b\\snapshots\\4782ad278652c7c71b72204d462d6d01eaaf7549\\tokenizer_config.json\n",
      "loading file tokenizer.json from cache at C:\\Users\\jinet\\.cache\\huggingface\\hub\\models--huggyllama--llama-7b\\snapshots\\4782ad278652c7c71b72204d462d6d01eaaf7549\\tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA] Train/Val sizes = 475/25\n",
      "[LoRA] Dataset tokenization debug salvato in lora_dataset_debug.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/475 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a897c2da622a46739ae4dab00ff7a3fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afaabe77eb1542bebf18ef276041da29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA] Device: cuda\n",
      "[LoRA] ⚙️  Preparazione caricamento modello…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\jinet\\.cache\\huggingface\\hub\\models--huggyllama--llama-7b\\snapshots\\4782ad278652c7c71b72204d462d6d01eaaf7549\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "CUDA backend validation successful.\n",
      "loading weights file model.safetensors from cache at C:\\Users\\jinet\\.cache\\huggingface\\hub\\models--huggyllama--llama-7b\\snapshots\\4782ad278652c7c71b72204d462d6d01eaaf7549\\model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "CUDA backend validation successful.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Lancia LoRA sul subset con batch_size piccolo\u001B[39;00m\n\u001B[0;32m      6\u001B[0m exp_dir \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexperiments/lora_subset\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 7\u001B[0m \u001B[43mrun_lora_train\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mSUBSET_FILE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mexp_dir\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhuggyllama/llama-7b\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m123\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_4bit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_bfloat16\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\n\u001B[0;32m     15\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\IdeaProjects\\EmoStyle\\training.py:143\u001B[0m, in \u001B[0;36mrun_lora_train\u001B[1;34m(train_file, output_dir, model_name, model_path, seed, use_4bit, batch_size, use_bfloat16)\u001B[0m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LoRA] ⚙️  Preparazione caricamento modello…\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    141\u001B[0m t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m--> 143\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mLlamaForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[43m    \u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    145\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbnb_config\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43muse_4bit\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    146\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbfloat16\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43muse_bfloat16\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat16\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[0;32m    149\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LoRA] ✅ from_pretrained in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtime\u001B[38;5;241m.\u001B[39mtime()\u001B[38;5;241m-\u001B[39mt0\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.1f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    151\u001B[0m t1 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\emostyle\\lib\\site-packages\\transformers\\modeling_utils.py:309\u001B[0m, in \u001B[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    307\u001B[0m old_dtype \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mget_default_dtype()\n\u001B[0;32m    308\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 309\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    310\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    311\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_default_dtype(old_dtype)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\emostyle\\lib\\site-packages\\transformers\\modeling_utils.py:4555\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   4553\u001B[0m \u001B[38;5;66;03m# Prepare the full device map\u001B[39;00m\n\u001B[0;32m   4554\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m device_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 4555\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m \u001B[43m_get_device_map\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_memory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4557\u001B[0m \u001B[38;5;66;03m# Finalize model weight initialization\u001B[39;00m\n\u001B[0;32m   4558\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m from_tf:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\emostyle\\lib\\site-packages\\transformers\\modeling_utils.py:1344\u001B[0m, in \u001B[0;36m_get_device_map\u001B[1;34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001B[0m\n\u001B[0;32m   1341\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m infer_auto_device_map(model, dtype\u001B[38;5;241m=\u001B[39mtarget_dtype, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdevice_map_kwargs)\n\u001B[0;32m   1343\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1344\u001B[0m         \u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_environment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1346\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m device_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1347\u001B[0m     tied_params \u001B[38;5;241m=\u001B[39m find_tied_parameters(model)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\emostyle\\lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:104\u001B[0m, in \u001B[0;36mBnb4BitHfQuantizer.validate_environment\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    102\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m    103\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map_without_lm_head\u001B[38;5;241m.\u001B[39mvalues() \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisk\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map_without_lm_head\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[1;32m--> 104\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    105\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    106\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    107\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    108\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`from_pretrained`. Check \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    109\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    110\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor more details. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    111\u001B[0m         )\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m version\u001B[38;5;241m.\u001B[39mparse(importlib\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mversion(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbitsandbytes\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \u001B[38;5;241m<\u001B[39m version\u001B[38;5;241m.\u001B[39mparse(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.39.0\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    115\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    116\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    117\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Plot delle curve di training vs validation\n",
    "\"\"\"\n",
    "Ogni `run_lora_train` salva il best adapter e la directory `output_dir`.\n",
    "Qui carichiamo il CSV di debug (`lora_dataset_debug.csv`) e il `loss_curve.png` generato.\n",
    "\"\"\"\n",
    "\n",
    "# Carica curve se presenti\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "img_path = exp_dir / \"loss_curve.png\"\n",
    "if img_path.exists():\n",
    "    img = mpimg.imread(str(img_path))\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "else:\n",
    "    print(\"Nessuna loss_curve.png trovata in\", img_path)\n"
   ],
   "id": "dadafb3e58ccf769"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. Param Sweeps: temperatura & lr\n",
    "\"\"\"\n",
    "Esempio di loop rapido su 2 temperature e 2 learning rate, per 1 epoca sola.\n",
    "\"\"\"\n",
    "\n",
    "temps = [0.5, 0.8]\n",
    "lrs   = [5e-5, 1e-4]\n",
    "results = {}\n",
    "\n",
    "for temp in temps:\n",
    "    for lr in lrs:\n",
    "        tag = f\"t{temp}_lr{lr}\"\n",
    "        out = Path(f\"experiments/sweep_{tag}\")\n",
    "        out.mkdir(parents=True, exist_ok=True)\n",
    "        # Modifica run_lora_train per passare temp e lr?\n",
    "        # Se non supportato, dovresti parametrizzare la funzione.\n",
    "        print(f\"→ Sweep {tag} (1 epoca)\")\n",
    "        # ipotetico run modificato:\n",
    "        run_lora_train(\n",
    "            train_file=SUBSET_FILE,\n",
    "            output_dir=str(out),\n",
    "            model_name=\"huggyllama/llama-7b\",\n",
    "            seed=123,\n",
    "            use_4bit=False,\n",
    "            batch_size=1\n",
    "            # lr e temp dovrebbero essere argomenti aggiuntivi\n",
    "        )\n",
    "        # poi estrai dai log il train/val loss finale e salvali in results[tag]\n"
   ],
   "id": "2a37bb44639c7c9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 📝 5. Conclusioni preliminari\n",
    "\n",
    "- **Subset training**: time per epoca ~ X minuti\n",
    "- **Curve**: trend di loss, over/underfitting\n",
    "- **Param Sweeps**: quale combo (temp, lr) dà loss più bassa?\n",
    "\n",
    "> *Qui annota direttamente nel notebook i tuoi commenti mentre guardi i grafici e i numeri.*\n"
   ],
   "id": "a827a2c71d3076d0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
